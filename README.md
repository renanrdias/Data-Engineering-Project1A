## The purpose of this project:
   * Practice data modeling using Postgres database
   * Create an ETL pipeline using python3+
   * Provide to analytics team the data in a format which is possible to manupulate and take insights from it about users' listening preferences.

----

## Processed files
#### There are two types of data set files tageted to be processed. Both types are in .json format:
   * song data sets
   * log data sets

#### Song Data sets
The song data sets contain metadata about songs and artists of that song. There is an example below how the data is organized in a .json file:<br/>
{<br/>"num_songs": 1,<br/> "artist_id": "ARJIE2Y1187B994AB7",<br/> "artist_latitude": null,<br/> "artist_longitude": null,<br/> "artist_location": "",<br/> "artist_name": "Line Renaud",<br/> "song_id": "SOUPIRU12A6D4FA1E1",<br/> "title": "Der Kleine Dompfaff <br/> "duration": 152.92036,<br/> "year": 0<br/>}

#### Log Data sets
The second dataset consists of log files in JSON format generated by [this event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.

The log files are partitioned by year and month.

----

## Libraries and dependencies to run locally:
#### It is recommended to use the package manager [pip](https://pip.pypa.io/en/stable/) to install the libraries and dependencies.

```bash
pip install psycopg2
pip install os
pip install glob
pip install pandas
```

----

## Run scripts:
After installing libraries and dependencies, run the first and second cell of test.ipynb by using ctrl + enter.

As soon as it is shown a following message: 'Connected: student@sparkifydb', run the third cell (ctrl+enter).

In order to check the tables, one can run the rest of cells according to the table to be checked.




